# 🚀 Simple & Fast Network-Based Web Crawler Framework

> **bs4 ❌, selenium ❌**  
> 오직 **`requests` 하나로** 네트워크 응답을 직접 호출해  
> **간편하고 · 쉽고 · 빠르게** 데이터를 크롤링하는 방법을 공유합니다.

---

## ✨ 프로젝트 소개

이 레포지토리는 웹 페이지를 **렌더링해서 긁는 방식이 아닌**,  
브라우저의 **Network 요청(Request / Response)** 을 분석해  
**API 엔드포인트를 직접 호출하는 크롤링 방식**을 다룹니다.

📌 즉,

- HTML 파싱 ❌  
- 브라우저 자동화 ❌  
- 느리고 불안정한 크롤링 ❌  

👉 **네트워크 응답(JSON)을 직접 받아오는 가장 효율적인 방법**에 집중합니다.

---

## 🧠 왜 이런 방식인가요?

대부분의 최신 웹 서비스는 실제로 이렇게 동작합니다 👇

1. 페이지 접속
2. 내부적으로 API 요청 발생
3. 서버가 **JSON 데이터**를 반환
4. 프론트엔드가 화면에 렌더링

💡 그렇다면 굳이 화면을 볼 필요가 있을까요?

👉 **우리는 2️⃣~3️⃣ 단계만 직접 호출하면 됩니다.**

---

## 🔥 이 방식의 장점

✅ **빠름**  
- 렌더링 과정 없음 → 속도 🚀

✅ **단순함**  
- `requests`만 알면 끝

✅ **안정적**  
- DOM 변경에 영향 없음

✅ **자동화 친화적**  
- 배치 작업, 파이프라인 구성에 최적

---

## 🛠 사용 기술 스택

- 🐍 **Python**
- 🌐 **requests**
- 📦 (선택) json / time / logging

> ❌ BeautifulSoup  
> ❌ Selenium  
> ❌ Playwright  

---

## 📂 예제에서 다루는 내용

- 🔍 Network 요청 분석 방법
- 🔑 Header / Params / Payload 구성
- 📄 JSON Response 파싱
- 🔁 Pagination / Cursor 기반 수집
- 🧱 에러 처리 & 재시도 전략

---

## 🎯 이런 분들께 추천해요

- 크롤링을 **빠르고 깔끔하게** 하고 싶은 분
- Selenium이 **너무 무겁게 느껴지는** 분
- 데이터 수집을 **파이프라인에 올리고 싶은** 분
- 실무에서 **재현 가능한 크롤링 방식**이 필요한 분

---

## ⚠️ 주의 사항

- robots.txt 및 서비스 약관을 반드시 확인하세요
- 과도한 요청은 서버에 부담을 줄 수 있습니다
- 학습 및 연구 목적 사용을 권장합니다

---

## 🙌 목표

> **“웹 크롤링을 더 단순하게, 더 빠르게, 더 실무적으로”**

이 레포가  
여러분의 데이터 수집 스트레스를 줄이는 데 도움이 되길 바랍니다 😊

---

⭐ 도움이 되었다면 Star 한 번 눌러주세요!
